[
  {
    "objectID": "content/01_journal/01_tidyverse.html",
    "href": "content/01_journal/01_tidyverse.html",
    "title": "Tidyverse",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh.\nThis is a .qmd file. It is plain text with special features. Any time you write just like this, it will be compiled to normal text in the website. If you put a # in front of your text, it will create a top level-header.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "content/01_journal/01_tidyverse.html#header-2",
    "href": "content/01_journal/01_tidyverse.html#header-2",
    "title": "Tidyverse",
    "section": "\n2.1 Header 2",
    "text": "2.1 Header 2\nHeader 3\nHeader 4\nHeader 5\nHeader 6"
  },
  {
    "objectID": "content/01_journal/02_data_acquisition.html",
    "href": "content/01_journal/02_data_acquisition.html",
    "title": "Data Acquisition",
    "section": "",
    "text": "library(ggplot2)\nlibrary(ggrepel)\nlibrary(tidyverse)\n\n#> ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#> ✔ dplyr     1.1.2     ✔ readr     2.1.4\n#> ✔ forcats   1.0.0     ✔ stringr   1.5.0\n#> ✔ lubridate 1.9.2     ✔ tibble    3.2.1\n#> ✔ purrr     1.0.1     ✔ tidyr     1.3.0\n#> ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::filter() masks stats::filter()\n#> ✖ dplyr::lag()    masks stats::lag()\n#> ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(workflows)\nlibrary(broom.mixed)\nlibrary(parsnip)\nlibrary(recipes)\n\n#> \n#> Attaching package: 'recipes'\n#> \n#> The following object is masked from 'package:stringr':\n#> \n#>     fixed\n#> \n#> The following object is masked from 'package:stats':\n#> \n#>     step\n\nlibrary(rsample)\nlibrary(yardstick)\n\n#> \n#> Attaching package: 'yardstick'\n#> \n#> The following object is masked from 'package:readr':\n#> \n#>     spec\n\nlibrary(rpart.plot)\n\n#> Loading required package: rpart\n\n# Read the data\nbike_features_tbl <- readRDS(\"C:/Users/mosta/Desktop/Business Decisions with Machine Learning/2/bike_features_tbl.rds\")\n\n# Remove missing values in the selected columns\nbike_features_tbl <- bike_features_tbl\n\n# Create a recipe\nbike_recipe <- recipe(price ~ category_2 + frame_material, data = bike_features_tbl) %>%\n  step_dummy(all_nominal(), -all_outcomes()) %>%\n  step_zv(all_predictors())\n\n# Prepare the recipe\nbike_recipe_prep <- prep(bike_recipe)\n\n# Split the data into training and test sets\nset.seed(1113)\nsplit_obj <- initial_split(bike_features_tbl, prop = 0.8, strata = \"category_2\")\ntrain_tbl <- training(split_obj)\ntest_tbl <- testing(split_obj)\n\n# Create a workflow for Model 01\nwf_model_01 <- workflow() %>%\n  add_model(linear_reg(mode = \"regression\") %>% set_engine(\"lm\")) %>%\n  add_recipe(bike_recipe_prep)\n\n# Fit Model 01 using the workflow\nmodel_01_linear_lm_simple <- fit(wf_model_01, data = train_tbl)\n\n# Make predictions on the test data for Model 01\npredictions_model_01 <- predict(model_01_linear_lm_simple, new_data = test_tbl) %>%\n  bind_cols(data.frame(price = test_tbl$price, category_2 = as.character(test_tbl$category_2)))\n\n#> Warning: There are new levels in a factor: E-Road\n\n# Calculate the RMSE manually for Model 01\nrmse_value_model_01 <- sqrt(mean((predictions_model_01$price - predictions_model_01$.pred)^2))\n\n# View the calculated RMSE for Model 01\nprint(rmse_value_model_01)\n\n#> [1] NA\n\n# Extract and tidy the coefficients for Model 01\ncoefs_model_01 <- broom.mixed::tidy(model_01_linear_lm_simple$fit$fit) %>%\n  arrange(p.value) %>%\n  mutate(term = as_factor(term) %>% fct_rev())\n\n# Plot the feature importance for Model 01\nggplot(coefs_model_01, aes(x = estimate, y = term)) +\n  geom_point(color = \"#2dc6d6\", size = 3) +\n  geom_label(aes(label = scales::dollar(coefs_model_01$estimate, accuracy = 1, suffix = \" €\", prefix = \"\")),\n             size = 3, fill = \"#272A36\", color = \"white\", hjust = 0) +\n  scale_x_continuous(labels = scales::dollar_format(suffix = \" €\", prefix = \"\")) +\n  labs(title = \"Linear Regression: Feature Importance\",\n       subtitle = \"Model 01: Simple lm Model\")\n\n#> Warning: Use of `coefs_model_01$estimate` is discouraged.\n#> ℹ Use `estimate` instead.\n\n\n\n\n\n\n\n# Define the helper function to calculate metrics\ncalc_metrics <- function(model, new_data = test_tbl) {\n  model %>%\n    predict(new_data = new_data) %>%\n    bind_cols(new_data %>% select(price)) %>%\n    yardstick::metrics(truth = price, estimate = .pred)\n}\n\n# Calculate the metrics using the helper function for Model 01\nmetrics_model_01 <- model_01_linear_lm_simple %>% calc_metrics(test_tbl)\n\n#> Warning: There are new levels in a factor: E-Road\n\n# Print the calculated metrics for Model 01\nprint(metrics_model_01)\n\n#> # A tibble: 3 × 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 rmse    standard    1053.   \n#> 2 rsq     standard       0.501\n#> 3 mae     standard     768.\n\n# Create a workflow for Model 02\nwf_model_02 <- workflow() %>%\n  add_model(linear_reg(mode = \"regression\") %>% set_engine(\"lm\")) %>%\n  add_recipe(bike_recipe_prep)\n\n# Fit Model 02 using the workflow\nmodel_02_linear_lm_complex <- fit(wf_model_02, data = train_tbl)\n\n# Make predictions on the test data for Model 02\npredictions_model_02 <- predict(model_02_linear_lm_complex, new_data = test_tbl) %>%\n  bind_cols(data.frame(price = test_tbl$price, category_2 = as.character(test_tbl$category_2)))\n\n#> Warning: There are new levels in a factor: E-Road\n\n# Calculate the RMSE manually for Model 02\nrmse_value_model_02 <- sqrt(mean((predictions_model_02$price - predictions_model_02$.pred)^2))\n\n# View the calculated RMSE for Model 02\nprint(rmse_value_model_02)\n\n#> [1] NA\n\n# Extract and tidy the coefficients for Model 02\ncoefs_model_02 <- tidy(model_02_linear_lm_complex$fit$fit) %>%\n  arrange(p.value) %>%\n  mutate(term = as_factor(term) %>% fct_rev())\n\n# Plot the feature importance for Model 02\nggplot(coefs_model_02, aes(x = estimate, y = term)) +\n  geom_point(color = \"#2dc6d6\", size = 3) +\n  ggrepel::geom_label_repel(aes(label = scales::dollar(estimate, accuracy = 1, suffix = \" €\", prefix = \"\")),\n                            size = 4, fill = \"#272A36\", color = \"white\") +\n  scale_x_continuous(labels = scales::dollar_format(suffix = \" €\", prefix = \"\")) +\n  labs(title = \"Linear Regression: Feature Importance\",\n       subtitle = \"Model 02: Complex lm Model\")\n\n\n\n\n\n\n# Calculate the metrics using the helper function for Model 02\nmetrics_model_02 <- model_02_linear_lm_complex %>% calc_metrics(test_tbl)\n\n#> Warning: There are new levels in a factor: E-Road\n\n# Print the calculated metrics for Model 02\nprint(metrics_model_02)\n\n#> # A tibble: 3 × 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 rmse    standard    1053.   \n#> 2 rsq     standard       0.501\n#> 3 mae     standard     768.\n\n# 5.0 TESTING THE ALGORITHMS OUT ----\ng1 <- bike_features_tbl %>% \n  mutate(category_2 = as.factor(category_2) %>% \n           fct_reorder(price)) %>% \n  \n  ggplot(aes(category_2, price)) +\n  geom_violin() +\n  geom_jitter(width = 0.1, alpha = 0.5, color = \"#2dc6d6\") +\n  coord_flip() +\n  facet_wrap(~ frame_material) +\n  scale_y_continuous(labels = scales::dollar_format()) +\n  labs(\n    title = \"Unit Price for Each Model\",\n    y = \"\", x = \"Category 2\"\n  )\n\n# Print the plot\nprint(g1)\n\n#> Warning: Groups with fewer than two data points have been dropped.\n\n\n#> Warning: Groups with fewer than two data points have been dropped.\n#> Groups with fewer than two data points have been dropped.\n\n\n\n\n\n\n\n# 5.1 NEW MODEL ----\n\nnew_cross_country <- tibble(\n  model = \"Exceed AL SL new\",\n  category_2 = \"Cross-Country\",\n  frame_material = \"aluminium\",\n  shimano_dura_ace = 0,\n  shimano_ultegra = 0,\n  shimano_105 = 0,\n  shimano_tiagra = 0,\n  Shimano_sora = 0,\n  shimano_deore = 0,\n  shimano_slx = 0,\n  shimano_grx = 0,\n  Shimano_xt = 1,\n  Shimano_xtr = 0,\n  Shimano_saint = 0,\n  SRAM_red = 0,\n  SRAM_force = 0,\n  SRAM_rival = 0,\n  SRAM_apex = 0,\n  SRAM_xx1 = 0,\n  SRAM_x01 = 0,\n  SRAM_gx = 0,\n  SRAM_nx = 0,\n  SRAM_sx = 0,\n  Campagnolo_potenza = 0,\n  Campagnolo_super_record = 0,\n  shimano_nexus = 0,\n  shimano_alfine = 0\n) \n\nnew_cross_country\n\n\n\n  \n\n\n# Linear Methods ----\n\n# Iteration\nmodels_tbl <- tibble(\n  model_id = str_c(\"Model 0\", 1:2),\n  model = list(\n    model_01_linear_lm_simple,\n    model_02_linear_lm_complex\n  )\n)\n\nmodels_tbl\n\n\n\n  \n\n\n# Add Predictions\npredictions_new_cross_country_tbl <- models_tbl %>%\n  mutate(predictions = map(model, predict, new_data = new_cross_country)) %>%\n  unnest(predictions) %>%\n  mutate(category_2 = \"Cross-Country\") %>%\n  left_join(new_cross_country, by = \"category_2\")\n\npredictions_new_cross_country_tbl\n\n\n\n  \n\n\n# Update plot\ng2 <- g1 +\n  geom_point(aes(y = .pred), color = \"red\", alpha = 0.5,\n             data = predictions_new_cross_country_tbl) +\n  ggrepel::geom_text_repel(aes(label = model_id, y = .pred),\n                           size = 3,\n                           data = predictions_new_cross_country_tbl)\nprint(g2)\n\n#> Warning: Groups with fewer than two data points have been dropped.\n#> Groups with fewer than two data points have been dropped.\n#> Groups with fewer than two data points have been dropped."
  },
  {
    "objectID": "content/01_journal/03_data_wrangling.html",
    "href": "content/01_journal/03_data_wrangling.html",
    "title": "Data Wrangling",
    "section": "",
    "text": "1 1\n\n##----.libraries----\nlibrary(tidyverse)\n\n#> ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#> ✔ dplyr     1.1.2     ✔ readr     2.1.4\n#> ✔ forcats   1.0.0     ✔ stringr   1.5.0\n#> ✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n#> ✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n#> ✔ purrr     1.0.1     \n#> ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::filter() masks stats::filter()\n#> ✖ dplyr::lag()    masks stats::lag()\n#> ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(skimr)\nlibrary(GGally)\n\n#> Registered S3 method overwritten by 'GGally':\n#>   method from   \n#>   +.gg   ggplot2\n\n##----.Load data-----\nemployee_attrition_tbl <- read_csv(\"C:/Users/mosta/Desktop/Business Decisions with Machine Learning/3/datasets-1067-1925-WA_Fn-UseC_-HR-Employee-Attrition.csv\")\n\n#> Rows: 1470 Columns: 35\n#> ── Column specification ────────────────────────────────────────────────────────\n#> Delimiter: \",\"\n#> chr  (9): Attrition, BusinessTravel, Department, EducationField, Gender, Job...\n#> dbl (26): Age, DailyRate, DistanceFromHome, Education, EmployeeCount, Employ...\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndept_job_role_tbl <- employee_attrition_tbl %>%\nselect(EmployeeNumber, Department, JobRole, PerformanceRating, Attrition)\n\n##----.Function to convert counts to percentages---- \ncount_to_pct <- function(data, ..., col = n) {\n  \n  # capture the dots\n  grouping_vars_expr <- quos(...)\n  col_expr <- enquo(col)\n  \n  ret <- data %>%\n    group_by(!!! grouping_vars_expr) %>%\n    mutate(pct = (!! col_expr) / sum(!! col_expr)) %>%\n    ungroup()\n  \n  return(ret)\n  \n}\n\n\n##----.Develop KPI function----\n\nassess_attrition <- function(data, attrition_col, attrition_value, baseline_pct) {\n  \n  attrition_col_expr <- enquo(attrition_col)\n  \n  data %>%\n    \n    # Use parenthesis () to give tidy eval evaluation priority\n    filter((!! attrition_col_expr) %in% attrition_value) %>%\n    arrange(desc(pct)) %>%\n    mutate(\n      # Function inputs in numeric format (e.g. baseline_pct = 0.088 don't require tidy eval)\n      above_industry_avg = case_when(\n        pct > baseline_pct ~ \"Yes\",\n        TRUE ~ \"No\"\n      )\n    )\n  \n}\n\n\n##----.Function to calculate attrition cost----\ncalculate_attrition_cost <- function(\n    \n  # Employee\n  n                    = 1,\n  salary               = 80000,\n  \n  # Direct Costs\n  separation_cost      = 500,\n  vacancy_cost         = 10000,\n  acquisition_cost     = 4900,\n  placement_cost       = 3500,\n  \n  # Productivity Costs\n  net_revenue_per_employee = 250000,\n  workdays_per_year        = 240,\n  workdays_position_open   = 40,\n  workdays_onboarding      = 60,\n  onboarding_efficiency    = 0.50\n  \n) {\n  \n  # Direct Costs\n  direct_cost <- sum(separation_cost, vacancy_cost, acquisition_cost, placement_cost)\n  \n  # Lost Productivity Costs\n  productivity_cost <- net_revenue_per_employee / workdays_per_year *\n    (workdays_position_open + workdays_onboarding * onboarding_efficiency)\n  \n  # Savings of Salary & Benefits (Cost Reduction)\n  salary_benefit_reduction <- salary / workdays_per_year * workdays_position_open\n  \n  # Estimated Turnover Per Employee\n  cost_per_employee <- direct_cost + productivity_cost - salary_benefit_reduction\n  \n  # Total Cost of Employee Turnover\n  total_cost <- n * cost_per_employee\n  \n  return(total_cost)\n  \n}\n\n\n##----.Function to plot attrition----\nplot_attrition <- function(data, \n                           ..., \n                           .value,\n                           fct_reorder = TRUE,\n                           fct_rev     = FALSE,\n                           include_lbl = TRUE,\n                           color       = \"#2dc6d6\",\n                           units       = c(\"0\", \"K\", \"M\")) {\n  \n  ### Inputs\n  group_vars_expr   <- quos(...)\n  \n  # If the user does not supply anything, \n  # this takes the first column of the supplied data\n  if (length(group_vars_expr) == 0) {\n    group_vars_expr <- quos(rlang::sym(colnames(data)[[1]]))\n  }\n  \n  value_expr <- enquo(.value)\n  \n  units_val  <- switch(units[[1]],\n                       \"M\" = 1e6,\n                       \"K\" = 1e3,\n                       \"0\" = 1)\n  if (units[[1]] == \"0\") units <- \"\"\n  \n  # Data Manipulation\n  # This is a so called Function Factory (a function that produces a function)\n  usd <- scales::dollar_format(prefix = \"$\", largest_with_cents = 1e3)\n  \n  # Create the axis labels and values for the plot\n  data_manipulated <- data %>%\n    mutate(name = str_c(!!! group_vars_expr, sep = \": \") %>% as_factor()) %>%\n    mutate(value_text = str_c(usd(!! value_expr / units_val),\n                              units[[1]], sep = \"\"))\n  \n  \n  # Order the labels on the y-axis according to the input\n  if (fct_reorder) {\n    data_manipulated <- data_manipulated %>%\n      mutate(name = forcats::fct_reorder(name, !! value_expr)) %>%\n      arrange(name)\n  }\n  \n  if (fct_rev) {\n    data_manipulated <- data_manipulated %>%\n      mutate(name = forcats::fct_rev(name)) %>%\n      arrange(name)\n  }\n  \n\n##----.Visualization----\n  \n  g <- data_manipulated %>%\n    \n    # \"name\" is a column name generated by our function internally as part of the data manipulation task\n    ggplot(aes(x = (!! value_expr), y = name)) +\n    geom_segment(aes(xend = 0, yend = name), color = color) +\n    geom_point(aes(size = !! value_expr), color = color) +\n    scale_x_continuous(labels = scales::dollar) +\n    scale_size(range = c(3, 5)) +\n    theme(legend.position = \"none\")\n  \n  # Plot labels if TRUE\n  if (include_lbl) {\n    g <- g +\n      geom_label(aes(label = value_text, size = !! value_expr),\n                 hjust = \"inward\", color = color)\n  }\n  \n  return(g)\n}\n\n##----.calculations----\nattrition_cost_df <- dept_job_role_tbl %>%\n  count(Department, JobRole, Attrition) %>%\n  count_to_pct(Department, JobRole)  %>%  \n  assess_attrition(Attrition, attrition_value = \"Yes\", baseline_pct = 0.088) %>%\n  mutate(\n    cost_of_attrition = calculate_attrition_cost(n = n, salary = 80000)\n  ) %>%\n  # Select columnns\n  plot_attrition(Department, JobRole, .value = cost_of_attrition,\n                 units = \"M\") +\n  labs(\n    title = \"Estimated Cost of Attrition by Job Role\",\n    x = \"Cost of Attrition\",\n    subtitle = \"Looks like Sales Executive and Labaratory Technician are the biggest drivers of cost\"\n  )\n  \n\n\n##----.Custom plotting function----\n\ndata <- employee_attrition_tbl %>%\n  select(Attrition, Age, Gender, MaritalStatus, NumCompaniesWorked, Over18, DistanceFromHome)\n\nplot_ggpairs <- function(data, color = NULL, density_alpha = 0.5) {\n  \n  color_expr <- enquo(color)\n  \n  if (rlang::quo_is_null(color_expr)) {\n    \n    g <- data %>%\n      ggpairs(lower = \"blank\") \n    \n  } else {\n    \n    color_name <- quo_name(color_expr)\n    \n    g <- data %>%\n      ggpairs(mapping = aes_string(color = color_name), \n              lower = \"blank\", legend = 1,\n              diag = list(continuous = wrap(\"densityDiag\", \n                                            alpha = density_alpha))) +\n      theme(legend.position = \"bottom\")\n  }\n  \n  return(g)\n  \n}\n\nemployee_attrition_tbl %>%\n  select(Attrition, Age, Gender, MaritalStatus, NumCompaniesWorked, Over18, DistanceFromHome) %>%\n  plot_ggpairs(color = Attrition)\n\n#> Warning: `aes_string()` was deprecated in ggplot2 3.0.0.\n#> ℹ Please use tidy evaluation idioms with `aes()`.\n#> ℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\n\n\n\n\n\n\n\n##----.Explore Features by Category----\n\n#   1. Descriptive features: age, gender, marital status \nemployee_attrition_tbl %>%\n  select(Attrition, Age, Gender, MaritalStatus, NumCompaniesWorked, Over18, DistanceFromHome) %>%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n#   2. Employment features: department, job role, job level\nemployee_attrition_tbl %>%\n  select(Attrition, contains(\"employee\"), contains(\"department\"), contains(\"job\")) %>%\n  plot_ggpairs(Attrition) \n\n#> Warning in cor(x, y): the standard deviation is zero\n\n\n#> Warning in cor(x, y): the standard deviation is zero\n\n#> Warning in cor(x, y): the standard deviation is zero\n\n#> Warning in cor(x, y): the standard deviation is zero\n\n#> Warning in cor(x, y): the standard deviation is zero\n\n#> Warning in cor(x, y): the standard deviation is zero\n\n#> Warning in cor(x, y): the standard deviation is zero\n\n#> Warning in cor(x, y): the standard deviation is zero\n\n#> Warning in cor(x, y): the standard deviation is zero\n\n#> Warning in cor(x, y): the standard deviation is zero\n\n#> Warning in cor(x, y): the standard deviation is zero\n\n#> Warning in cor(x, y): the standard deviation is zero\n\n\n\n\n\n\n\n#   3. Compensation features: HourlyRate, MonthlyIncome, StockOptionLevel \nemployee_attrition_tbl %>%\n  select(Attrition, contains(\"income\"), contains(\"rate\"), contains(\"salary\"), contains(\"stock\")) %>%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n#   4. Survey Results: Satisfaction level, WorkLifeBalance \nemployee_attrition_tbl %>%\n  select(Attrition, contains(\"satisfaction\"), contains(\"life\")) %>%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n#   5. Performance Data: Job Involvement, Performance Rating\nemployee_attrition_tbl %>%\n  select(Attrition, contains(\"performance\"), contains(\"involvement\")) %>%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n#   6. Work-Life Features \nemployee_attrition_tbl %>%\n  select(Attrition, contains(\"overtime\"), contains(\"travel\")) %>%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n#   7. Training and Education \nemployee_attrition_tbl %>%\n  select(Attrition, contains(\"training\"), contains(\"education\")) %>%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n#   8. Time-Based Features: Years at company, years in current role\nemployee_attrition_tbl %>%\n  select(Attrition, contains(\"years\")) %>%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n\n\n2 2\n\n# Load the necessary libraries\nlibrary(h2o)\n\n#> \n#> ----------------------------------------------------------------------\n#> \n#> Your next step is to start H2O:\n#>     > h2o.init()\n#> \n#> For H2O package documentation, ask for help:\n#>     > ??h2o\n#> \n#> After starting H2O, you can use the Web UI at http://localhost:54321\n#> For more information visit https://docs.h2o.ai\n#> \n#> ----------------------------------------------------------------------\n\n\n#> \n#> Attaching package: 'h2o'\n\n\n#> The following objects are masked from 'package:lubridate':\n#> \n#>     day, hour, month, week, year\n\n\n#> The following objects are masked from 'package:stats':\n#> \n#>     cor, sd, var\n\n\n#> The following objects are masked from 'package:base':\n#> \n#>     %*%, %in%, &&, ||, apply, as.factor, as.numeric, colnames,\n#>     colnames<-, ifelse, is.character, is.factor, is.numeric, log,\n#>     log10, log1p, log2, round, signif, trunc\n\nlibrary(magrittr)\n\n#> \n#> Attaching package: 'magrittr'\n\n\n#> The following object is masked from 'package:purrr':\n#> \n#>     set_names\n\n\n#> The following object is masked from 'package:tidyr':\n#> \n#>     extract\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(tibble)\nlibrary(stringr)\nlibrary(forcats)\n\n# Initialize H2O\nh2o.init()\n\n#>  Connection successful!\n#> \n#> R is connected to the H2O cluster: \n#>     H2O cluster uptime:         48 minutes 2 seconds \n#>     H2O cluster timezone:       Europe/Berlin \n#>     H2O data parsing timezone:  UTC \n#>     H2O cluster version:        3.40.0.4 \n#>     H2O cluster version age:    1 month and 1 day \n#>     H2O cluster name:           H2O_started_from_R_mosta_xsx147 \n#>     H2O cluster total nodes:    1 \n#>     H2O cluster total memory:   3.17 GB \n#>     H2O cluster total cores:    12 \n#>     H2O cluster allowed cores:  12 \n#>     H2O cluster healthy:        TRUE \n#>     H2O Connection ip:          localhost \n#>     H2O Connection port:        54321 \n#>     H2O Connection proxy:       NA \n#>     H2O Internal Security:      FALSE \n#>     R Version:                  R version 4.2.3 (2023-03-15 ucrt)\n\n# Load the training, validation, and test datasets\ntrain_df <- h2o.importFile(path = \"C:/Users/mosta/Desktop/Business Decisions with Machine Learning/3/product_backorders.csv\")\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\nvalid_df <- h2o.importFile(path = \"C:/Users/mosta/Desktop/Business Decisions with Machine Learning/3/product_backorders.csv\")\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\ntest_df <- h2o.importFile(path = \"C:/Users/mosta/Desktop/Business Decisions with Machine Learning/3/product_backorders.csv\")\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n# Specify the response and predictor variables\nresponse <- \"went_on_backorder\"\npredictors <- setdiff(names(train_df), response)\n\n# Run AutoML specifying the stopping criterion\nautoml_models_h2o <- h2o.automl(\n  x = predictors,\n  y = response,\n  training_frame = train_df,\n  validation_frame = valid_df,\n  max_runtime_secs = 60\n)\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   2%\n#> 23:58:14.158: User specified a validation frame with cross-validation still enabled. Please note that the models will still be validated using cross-validation only, the validation frame will be used to provide purely informative validation metrics on the trained models.\n#> 23:58:14.158: AutoML: XGBoost is not available; skipping it.\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |==========================================                            |  59%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |===============================================================       |  89%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |======================================================================| 100%\n\n# View the leaderboard\nleaderboard <- automl_models_h2o@leaderboard\nprint(leaderboard)\n\n#>                                                   model_id       auc   logloss\n#> 1    StackedEnsemble_AllModels_2_AutoML_10_20230529_235814 0.9558295 0.1636339\n#> 2 StackedEnsemble_BestOfFamily_3_AutoML_10_20230529_235814 0.9554741 0.1646816\n#> 3    StackedEnsemble_AllModels_1_AutoML_10_20230529_235814 0.9552705 0.1645939\n#> 4 StackedEnsemble_BestOfFamily_2_AutoML_10_20230529_235814 0.9544588 0.1668605\n#> 5                          GBM_4_AutoML_10_20230529_235814 0.9535208 0.1682149\n#> 6                          GBM_3_AutoML_10_20230529_235814 0.9521810 0.1705099\n#>       aucpr mean_per_class_error      rmse        mse\n#> 1 0.7628844            0.1317766 0.2209082 0.04880042\n#> 2 0.7601398            0.1355058 0.2214359 0.04903386\n#> 3 0.7616456            0.1455501 0.2217644 0.04917945\n#> 4 0.7550740            0.1377233 0.2231046 0.04977568\n#> 5 0.7540215            0.1385044 0.2238265 0.05009830\n#> 6 0.7481104            0.1466169 0.2247535 0.05051412\n#> \n#> [15 rows x 7 columns]\n\n# Extract the model from the leaderboard\nmodel <- automl_models_h2o@leaderboard[6, \"model_id\"] %>%\n  h2o.getModel()\n\n# Extract the leader model\nleader_model <- automl_models_h2o@leader\n\n# Predict using the leader model\npredictions <- h2o.predict(leader_model, newdata = test_df)\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\npredictions_tbl <- as_tibble(predictions)\npredictions_tbl\n\n\n\n  \n\n\n# Set the directory path to save the leader model\nsave_directory <- \"C:/Users/mosta/Desktop/Business Decisions with Machine Learning/3/leadermodel\"\n\n# Save the leader model\nh2o.saveModel(leader_model, path = save_directory, force = TRUE)\n\n#> [1] \"C:\\\\Users\\\\mosta\\\\Desktop\\\\Business Decisions with Machine Learning\\\\3\\\\leadermodel\\\\StackedEnsemble_AllModels_2_AutoML_10_20230529_235814\"\n\nh2o.saveModel(model, path = save_directory, force = TRUE)\n\n#> [1] \"C:\\\\Users\\\\mosta\\\\Desktop\\\\Business Decisions with Machine Learning\\\\3\\\\leadermodel\\\\GBM_3_AutoML_10_20230529_235814\"\n\n# Visualize the leaderboard\nplot_h2o_leaderboard <- function(h2o_leaderboard, order_by = c(\"auc\", \"logloss\"),\n                                 n_max = 20, size = 4, include_lbl = TRUE) {\n  # Setup inputs\n  order_by <- tolower(order_by[[1]])\n  \n  leaderboard_tbl <- h2o_leaderboard %>%\n    as_tibble() %>%\n    select(-c(aucpr, mean_per_class_error, rmse, mse)) %>% \n    mutate(model_type = str_extract(model_id, \"[^_]+\")) %>%\n    rownames_to_column(var = \"rowname\") %>%\n    mutate(model_id = paste0(rowname, \". \", model_id) %>% as.factor())\n  \n  # Transformation\n  if (order_by == \"auc\") {\n    data_transformed_tbl <- leaderboard_tbl %>%\n      slice(1:n_max) %>%\n      mutate(\n        model_id = as_factor(model_id) %>% reorder(auc),\n        model_type = as.factor(model_type)\n      ) %>%\n      pivot_longer(cols = -c(model_id, model_type, rowname), \n                   names_to = \"key\", \n                   values_to = \"value\", \n                   names_transform = list(key = forcats::fct_inorder)\n      )\n  } else if (order_by == \"logloss\") {\n    data_transformed_tbl <- leaderboard_tbl %>%\n      slice(1:n_max) %>%\n      mutate(\n        model_id = as_factor(model_id) %>% reorder(logloss) %>% fct_rev(),\n        model_type = as.factor(model_type)\n      ) %>%\n      pivot_longer(cols = -c(model_id, model_type, rowname), \n                   names_to = \"key\", \n                   values_to = \"value\", \n                   names_transform = list(key = forcats::fct_inorder)\n      )\n  } else {\n    # If nothing is supplied\n    stop(paste0(\"order_by = '\", order_by, \"' is not a permitted option.\"))\n  }\n  \n  # Visualization\n  g <- data_transformed_tbl %>%\n    ggplot(aes(value, model_id, color = model_type)) +\n    geom_point(size = size) +\n    facet_wrap(~ key, scales = \"free_x\") +\n    labs(title = \"Leaderboard Metrics\",\n         subtitle = paste0(\"Ordered by: \", toupper(order_by)),\n         y = \"Model Position, Model ID\", x = \"\")\n  \n  if (include_lbl) g <- g + geom_label(aes(label = round(value, 2), hjust = \"inward\"))\n  \n  return(g)\n}\n\nplot_h2o_leaderboard(automl_models_h2o@leaderboard, order_by = \"auc\", n_max = 15)\n\n\n\n\n\n\n\n\n3 3\n\n# Load the necessary libraries\nlibrary(h2o)\nlibrary(magrittr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(tibble)\nlibrary(stringr)\nlibrary(forcats)\n\n# Initialize H2O\nh2o.init()\n\n#>  Connection successful!\n#> \n#> R is connected to the H2O cluster: \n#>     H2O cluster uptime:         49 minutes 27 seconds \n#>     H2O cluster timezone:       Europe/Berlin \n#>     H2O data parsing timezone:  UTC \n#>     H2O cluster version:        3.40.0.4 \n#>     H2O cluster version age:    1 month and 1 day \n#>     H2O cluster name:           H2O_started_from_R_mosta_xsx147 \n#>     H2O cluster total nodes:    1 \n#>     H2O cluster total memory:   3.14 GB \n#>     H2O cluster total cores:    12 \n#>     H2O cluster allowed cores:  12 \n#>     H2O cluster healthy:        TRUE \n#>     H2O Connection ip:          localhost \n#>     H2O Connection port:        54321 \n#>     H2O Connection proxy:       NA \n#>     H2O Internal Security:      FALSE \n#>     R Version:                  R version 4.2.3 (2023-03-15 ucrt)\n\n# Load the training, validation, and test datasets\ntrain_df <- h2o.importFile(path = \"C:/Users/mosta/Desktop/Business Decisions with Machine Learning/3/product_backorders.csv\")\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\nvalid_df <- h2o.importFile(path = \"C:/Users/mosta/Desktop/Business Decisions with Machine Learning/3/product_backorders.csv\")\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\ntest_df <- h2o.importFile(path = \"C:/Users/mosta/Desktop/Business Decisions with Machine Learning/3/product_backorders.csv\")\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n# Specify the response and predictor variables\nresponse <- \"went_on_backorder\"\npredictors <- setdiff(names(train_df), response)\n\n# Run AutoML specifying the stopping criterion\nautoml_models_h2o <- h2o.automl(\n  x = predictors,\n  y = response,\n  training_frame = train_df,\n  validation_frame = valid_df,\n  max_runtime_secs = 60\n)\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   2%\n#> 23:59:39.915: User specified a validation frame with cross-validation still enabled. Please note that the models will still be validated using cross-validation only, the validation frame will be used to provide purely informative validation metrics on the trained models.\n#> 23:59:39.915: AutoML: XGBoost is not available; skipping it.\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |==============                                                        |  19%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |===================                                                   |  26%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |=======================                                               |  34%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |=================================================                     |  69%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |======================================================                |  76%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |======================================================================| 100%\n\n# View the leaderboard\nleaderboard <- automl_models_h2o@leaderboard\nprint(leaderboard)\n\n#>                                                   model_id       auc   logloss\n#> 1    StackedEnsemble_AllModels_2_AutoML_11_20230529_235939 0.9557123 0.1640563\n#> 2    StackedEnsemble_AllModels_1_AutoML_11_20230529_235939 0.9555524 0.1644311\n#> 3 StackedEnsemble_BestOfFamily_3_AutoML_11_20230529_235939 0.9551357 0.1665577\n#> 4 StackedEnsemble_BestOfFamily_2_AutoML_11_20230529_235939 0.9545714 0.1680479\n#> 5                          GBM_4_AutoML_11_20230529_235939 0.9535397 0.1696111\n#> 6 StackedEnsemble_BestOfFamily_1_AutoML_11_20230529_235939 0.9520433 0.1684072\n#>       aucpr mean_per_class_error      rmse        mse\n#> 1 0.7630398            0.1450911 0.2214337 0.04903287\n#> 2 0.7621662            0.1490994 0.2220009 0.04928439\n#> 3 0.7553260            0.1347563 0.2228581 0.04966575\n#> 4 0.7512539            0.1441818 0.2244072 0.05035858\n#> 5 0.7494295            0.1386806 0.2252960 0.05075829\n#> 6 0.7541630            0.1432870 0.2245253 0.05041163\n#> \n#> [14 rows x 7 columns]\n\n# Extract the model from the leaderboard\nmodel <- automl_models_h2o@leaderboard[6, \"model_id\"] %>%\n  h2o.getModel()\n\n# Extract the leader model\nleader_model <- automl_models_h2o@leader\n\n# Predict using the leader model\npredictions <- h2o.predict(leader_model, newdata = test_df)\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\npredictions_tbl <- as_tibble(predictions)\npredictions_tbl\n\n\n\n  \n\n\n# Set the directory path to save the leader model\nsave_directory <- \"C:/Users/mosta/Desktop/Business Decisions with Machine Learning/3/leadermodel\"\n\n# Save the leader model\nh2o.saveModel(leader_model, path = save_directory, force = TRUE)\n\n#> [1] \"C:\\\\Users\\\\mosta\\\\Desktop\\\\Business Decisions with Machine Learning\\\\3\\\\leadermodel\\\\StackedEnsemble_AllModels_2_AutoML_11_20230529_235939\"\n\nh2o.saveModel(model, path = save_directory, force = TRUE)\n\n#> [1] \"C:\\\\Users\\\\mosta\\\\Desktop\\\\Business Decisions with Machine Learning\\\\3\\\\leadermodel\\\\StackedEnsemble_BestOfFamily_1_AutoML_11_20230529_235939\"\n\n# Remove the leader model\nleader_model <- NULL\n\n# Define a unique grid_id using the current timestamp\ngrid_id_unique <- paste0(\"deeplearning_grid_\", format(Sys.time(), \"%Y%m%d%H%M%S\"))\n\n# Perform grid search with deeplearning algorithm\ngrid_search <- h2o.grid(\n  algorithm = \"deeplearning\",\n  grid_id = grid_id_unique,  # Use unique grid_id\n  x = predictors,\n  y = response,\n  training_frame = train_df,\n  validation_frame = valid_df,\n  hyper_params = list(\n    hidden = list(c(10, 10, 10), c(50, 20, 10), c(20, 20, 20)),\n    epochs = c(10, 50, 100)\n  )\n)\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n# Get the best model from the grid search\ngrid_models <- h2o.getGrid(grid_search@grid_id)\nbest_model <- h2o.getModel(grid_models@model_ids[[1]])\n\n# Predict using the best model\npredictions_best <- h2o.predict(best_model, newdata = test_df)\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\npredictions_best_tbl <- as_tibble(predictions_best)\npredictions_best_tbl\n\n\n\n  \n\n\n\n\n4 4\n\n# Load the necessary libraries\nlibrary(h2o)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tibble)\n\n# Initialize H2O\nh2o.init()\n\n#>  Connection successful!\n#> \n#> R is connected to the H2O cluster: \n#>     H2O cluster uptime:         51 minutes 27 seconds \n#>     H2O cluster timezone:       Europe/Berlin \n#>     H2O data parsing timezone:  UTC \n#>     H2O cluster version:        3.40.0.4 \n#>     H2O cluster version age:    1 month and 1 day \n#>     H2O cluster name:           H2O_started_from_R_mosta_xsx147 \n#>     H2O cluster total nodes:    1 \n#>     H2O cluster total memory:   3.08 GB \n#>     H2O cluster total cores:    12 \n#>     H2O cluster allowed cores:  12 \n#>     H2O cluster healthy:        TRUE \n#>     H2O Connection ip:          localhost \n#>     H2O Connection port:        54321 \n#>     H2O Connection proxy:       NA \n#>     H2O Internal Security:      FALSE \n#>     R Version:                  R version 4.2.3 (2023-03-15 ucrt)\n\n# Load the training, validation, and test datasets\ntrain_df <- h2o.importFile(path = \"C:/Users/mosta/Desktop/Business Decisions with Machine Learning/3/product_backorders.csv\")\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\nvalid_df <- h2o.importFile(path = \"C:/Users/mosta/Desktop/Business Decisions with Machine Learning/3/product_backorders.csv\")\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\ntest_df <- h2o.importFile(path = \"C:/Users/mosta/Desktop/Business Decisions with Machine Learning/3/product_backorders.csv\")\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n# Specify the response and predictor variables\nresponse <- \"went_on_backorder\"\npredictors <- setdiff(names(train_df), response)\n\n# Run AutoML specifying the stopping criterion\nautoml_models_h2o <- h2o.automl(\n  x = predictors,\n  y = response,\n  training_frame = train_df,\n  validation_frame = valid_df,\n  max_runtime_secs = 60\n)\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   2%\n#> 00:01:39.386: User specified a validation frame with cross-validation still enabled. Please note that the models will still be validated using cross-validation only, the validation frame will be used to provide purely informative validation metrics on the trained models.\n#> 00:01:39.386: AutoML: XGBoost is not available; skipping it.\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |=============================================                         |  65%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |======================================================================| 100%\n\n# Extract the leader model\nleader_model <- automl_models_h2o@leader\n\n# Predict using the leader model\npredictions <- h2o.predict(leader_model, newdata = test_df)\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\npredictions_tbl <- as_tibble(predictions)\n\n# Set the directory path to save the leader model\nsave_directory <- \"C:/Users/mosta/Desktop/Business Decisions with Machine Learning/3/leadermodel\"\n\n# Save the leader model\nh2o.saveModel(leader_model, path = save_directory, force = TRUE)\n\n#> [1] \"C:\\\\Users\\\\mosta\\\\Desktop\\\\Business Decisions with Machine Learning\\\\3\\\\leadermodel\\\\StackedEnsemble_AllModels_2_AutoML_12_20230530_00139\"\n\n# Define new theme\ntheme_new <- theme(\n  legend.position  = \"bottom\",\n  legend.key       = element_blank(),\n  panel.background = element_rect(fill   = \"transparent\"),\n  panel.border     = element_rect(color = \"black\", fill = NA, size = 0.5),\n  panel.grid.major = element_line(color = \"grey\", size = 0.333)\n)\n\n#> Warning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\n#> ℹ Please use the `linewidth` argument instead.\n\n\n#> Warning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\n#> ℹ Please use the `linewidth` argument instead.\n\n# Get the performance object for the leader model on the test set\nperf <- h2o.performance(leader_model, newdata = test_df)\n\n# Create a data frame of the recall and precision\npr_df <- data.frame(\n  threshold = perf@metrics$thresholds_and_metric_scores$threshold,\n  precision = perf@metrics$thresholds_and_metric_scores$precision,\n  recall = perf@metrics$thresholds_and_metric_scores$recall\n)\n\n# Filter out any NA rows\npr_df <- pr_df[complete.cases(pr_df), ]\n\n# Find the threshold that maximizes f1\noptimal_threshold <- h2o.find_threshold_by_max_metric(perf, \"f1\")\n\n# Plot the precision-recall curve with new theme\npr_df %>%\n  ggplot(aes(x = threshold)) +\n  geom_line(aes(y = precision), color = \"blue\", size = 1) +\n  geom_line(aes(y = recall), color = \"red\", size = 1) +\n  geom_vline(xintercept = optimal_threshold) +\n  labs(title = \"Precision vs Recall\", y = \"value\") +\n  theme_new\n\n#> Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n#> ℹ Please use `linewidth` instead."
  },
  {
    "objectID": "content/01_journal/04_data_visualization.html",
    "href": "content/01_journal/04_data_visualization.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/02_notes/05_class_notes.html",
    "href": "content/02_notes/05_class_notes.html",
    "title": "Class Notes",
    "section": "",
    "text": "IMPORTANT: You can delete everything in here and start fresh. You might want to start by not deleting anything above this line until you know what that stuff is doing.\nThis is an .qmd file. It is plain text with special features. Any time you write just like this, it will be compiled to normal text in the website. If you put a # in front of your text, it will create a top level-header."
  },
  {
    "objectID": "content/03_other/06_links.html",
    "href": "content/03_other/06_links.html",
    "title": "Links",
    "section": "",
    "text": "R is a free open-source programming language that can be used for statistical analysis, data-simulation, graphing, and lots of other stuff. Another free program is R-studio, that provides a nice graphic interface for R. Download R first, then download R-studio. Both can run on PCs, Macs or Linux. Students will be learning R in the stats labs using the lab manual .\n\n\n\n\nGoogle is great, Google your problem\nStackoverflow is great, google will often take you there because someone has already asked your question, and someone else has answered, usually many people have answered your question many ways."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Lab Journal",
    "section": "",
    "text": "This is a template example for lab journaling. Students in the data science courses at the Institute of Entrepreneurship will use this template to learn R for business analytics. Students can replace this text as they wish."
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "My Lab Journal",
    "section": "How to use",
    "text": "How to use\n\nAccept the assignment and get your own github repo.\nBlog/journal what you are doing in R, by editing the .qmd files.\nSee the links page for lots of helpful links on learning R.\nChange everything to make it your own.\nMake sure to render you website everytime before you want to upload changes"
  }
]