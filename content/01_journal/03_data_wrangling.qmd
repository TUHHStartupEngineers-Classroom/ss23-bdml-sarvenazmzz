---
title: "Data Wrangling"
author: "Joschka Schwarz"
---

# 1
```{r}
##----.libraries----
library(tidyverse)
library(dplyr)
library(ggplot2)
library(tidyverse)
library(readxl)
library(skimr)
library(GGally)

##----.Load data-----
employee_attrition_tbl <- read_csv("C:/Users/mosta/Desktop/Business Decisions with Machine Learning/3/datasets-1067-1925-WA_Fn-UseC_-HR-Employee-Attrition.csv")
dept_job_role_tbl <- employee_attrition_tbl %>%
select(EmployeeNumber, Department, JobRole, PerformanceRating, Attrition)

##----.Function to convert counts to percentages---- 
count_to_pct <- function(data, ..., col = n) {
  
  # capture the dots
  grouping_vars_expr <- quos(...)
  col_expr <- enquo(col)
  
  ret <- data %>%
    group_by(!!! grouping_vars_expr) %>%
    mutate(pct = (!! col_expr) / sum(!! col_expr)) %>%
    ungroup()
  
  return(ret)
  
}


##----.Develop KPI function----

assess_attrition <- function(data, attrition_col, attrition_value, baseline_pct) {
  
  attrition_col_expr <- enquo(attrition_col)
  
  data %>%
    
    # Use parenthesis () to give tidy eval evaluation priority
    filter((!! attrition_col_expr) %in% attrition_value) %>%
    arrange(desc(pct)) %>%
    mutate(
      # Function inputs in numeric format (e.g. baseline_pct = 0.088 don't require tidy eval)
      above_industry_avg = case_when(
        pct > baseline_pct ~ "Yes",
        TRUE ~ "No"
      )
    )
  
}


##----.Function to calculate attrition cost----
calculate_attrition_cost <- function(
    
  # Employee
  n                    = 1,
  salary               = 80000,
  
  # Direct Costs
  separation_cost      = 500,
  vacancy_cost         = 10000,
  acquisition_cost     = 4900,
  placement_cost       = 3500,
  
  # Productivity Costs
  net_revenue_per_employee = 250000,
  workdays_per_year        = 240,
  workdays_position_open   = 40,
  workdays_onboarding      = 60,
  onboarding_efficiency    = 0.50
  
) {
  
  # Direct Costs
  direct_cost <- sum(separation_cost, vacancy_cost, acquisition_cost, placement_cost)
  
  # Lost Productivity Costs
  productivity_cost <- net_revenue_per_employee / workdays_per_year *
    (workdays_position_open + workdays_onboarding * onboarding_efficiency)
  
  # Savings of Salary & Benefits (Cost Reduction)
  salary_benefit_reduction <- salary / workdays_per_year * workdays_position_open
  
  # Estimated Turnover Per Employee
  cost_per_employee <- direct_cost + productivity_cost - salary_benefit_reduction
  
  # Total Cost of Employee Turnover
  total_cost <- n * cost_per_employee
  
  return(total_cost)
  
}


##----.Function to plot attrition----
plot_attrition <- function(data, 
                           ..., 
                           .value,
                           fct_reorder = TRUE,
                           fct_rev     = FALSE,
                           include_lbl = TRUE,
                           color       = "#2dc6d6",
                           units       = c("0", "K", "M")) {
  
  ### Inputs
  group_vars_expr   <- quos(...)
  
  # If the user does not supply anything, 
  # this takes the first column of the supplied data
  if (length(group_vars_expr) == 0) {
    group_vars_expr <- quos(rlang::sym(colnames(data)[[1]]))
  }
  
  value_expr <- enquo(.value)
  
  units_val  <- switch(units[[1]],
                       "M" = 1e6,
                       "K" = 1e3,
                       "0" = 1)
  if (units[[1]] == "0") units <- ""
  
  # Data Manipulation
  # This is a so called Function Factory (a function that produces a function)
  usd <- scales::dollar_format(prefix = "$", largest_with_cents = 1e3)
  
  # Create the axis labels and values for the plot
  data_manipulated <- data %>%
    mutate(name = str_c(!!! group_vars_expr, sep = ": ") %>% as_factor()) %>%
    mutate(value_text = str_c(usd(!! value_expr / units_val),
                              units[[1]], sep = ""))
  
  
  # Order the labels on the y-axis according to the input
  if (fct_reorder) {
    data_manipulated <- data_manipulated %>%
      mutate(name = forcats::fct_reorder(name, !! value_expr)) %>%
      arrange(name)
  }
  
  if (fct_rev) {
    data_manipulated <- data_manipulated %>%
      mutate(name = forcats::fct_rev(name)) %>%
      arrange(name)
  }
  

##----.Visualization----
  
  g <- data_manipulated %>%
    
    # "name" is a column name generated by our function internally as part of the data manipulation task
    ggplot(aes(x = (!! value_expr), y = name)) +
    geom_segment(aes(xend = 0, yend = name), color = color) +
    geom_point(aes(size = !! value_expr), color = color) +
    scale_x_continuous(labels = scales::dollar) +
    scale_size(range = c(3, 5)) +
    theme(legend.position = "none")
  
  # Plot labels if TRUE
  if (include_lbl) {
    g <- g +
      geom_label(aes(label = value_text, size = !! value_expr),
                 hjust = "inward", color = color)
  }
  
  return(g)
}

##----.calculations----
attrition_cost_df <- dept_job_role_tbl %>%
  count(Department, JobRole, Attrition) %>%
  count_to_pct(Department, JobRole)  %>%  
  assess_attrition(Attrition, attrition_value = "Yes", baseline_pct = 0.088) %>%
  mutate(
    cost_of_attrition = calculate_attrition_cost(n = n, salary = 80000)
  ) %>%
  # Select columnns
  plot_attrition(Department, JobRole, .value = cost_of_attrition,
                 units = "M") +
  labs(
    title = "Estimated Cost of Attrition by Job Role",
    x = "Cost of Attrition",
    subtitle = "Looks like Sales Executive and Labaratory Technician are the biggest drivers of cost"
  )
  


##----.Custom plotting function----

data <- employee_attrition_tbl %>%
  select(Attrition, Age, Gender, MaritalStatus, NumCompaniesWorked, Over18, DistanceFromHome)

plot_ggpairs <- function(data, color = NULL, density_alpha = 0.5) {
  
  color_expr <- enquo(color)
  
  if (rlang::quo_is_null(color_expr)) {
    
    g <- data %>%
      ggpairs(lower = "blank") 
    
  } else {
    
    color_name <- quo_name(color_expr)
    
    g <- data %>%
      ggpairs(mapping = aes_string(color = color_name), 
              lower = "blank", legend = 1,
              diag = list(continuous = wrap("densityDiag", 
                                            alpha = density_alpha))) +
      theme(legend.position = "bottom")
  }
  
  return(g)
  
}

employee_attrition_tbl %>%
  select(Attrition, Age, Gender, MaritalStatus, NumCompaniesWorked, Over18, DistanceFromHome) %>%
  plot_ggpairs(color = Attrition)


##----.Explore Features by Category----

#   1. Descriptive features: age, gender, marital status 
employee_attrition_tbl %>%
  select(Attrition, Age, Gender, MaritalStatus, NumCompaniesWorked, Over18, DistanceFromHome) %>%
  plot_ggpairs(Attrition)

#   2. Employment features: department, job role, job level
employee_attrition_tbl %>%
  select(Attrition, contains("employee"), contains("department"), contains("job")) %>%
  plot_ggpairs(Attrition) 

#   3. Compensation features: HourlyRate, MonthlyIncome, StockOptionLevel 
employee_attrition_tbl %>%
  select(Attrition, contains("income"), contains("rate"), contains("salary"), contains("stock")) %>%
  plot_ggpairs(Attrition)

#   4. Survey Results: Satisfaction level, WorkLifeBalance 
employee_attrition_tbl %>%
  select(Attrition, contains("satisfaction"), contains("life")) %>%
  plot_ggpairs(Attrition)

#   5. Performance Data: Job Involvement, Performance Rating
employee_attrition_tbl %>%
  select(Attrition, contains("performance"), contains("involvement")) %>%
  plot_ggpairs(Attrition)

#   6. Work-Life Features 
employee_attrition_tbl %>%
  select(Attrition, contains("overtime"), contains("travel")) %>%
  plot_ggpairs(Attrition)

#   7. Training and Education 
employee_attrition_tbl %>%
  select(Attrition, contains("training"), contains("education")) %>%
  plot_ggpairs(Attrition)

#   8. Time-Based Features: Years at company, years in current role
employee_attrition_tbl %>%
  select(Attrition, contains("years")) %>%
  plot_ggpairs(Attrition)

```


# 2

```{r plot ,fig.width=10 , fig.height=7}
# Load the necessary libraries
library(h2o)
library(magrittr)
library(dplyr)
library(ggplot2)
library(tidyr)
library(tibble)
library(stringr)
library(forcats)

# Initialize H2O
h2o.init()

# Load the training, validation, and test datasets
train_df <- h2o.importFile(path = "C:/Users/mosta/Desktop/Business Decisions with Machine Learning/3/product_backorders.csv")
valid_df <- h2o.importFile(path = "C:/Users/mosta/Desktop/Business Decisions with Machine Learning/3/product_backorders.csv")
test_df <- h2o.importFile(path = "C:/Users/mosta/Desktop/Business Decisions with Machine Learning/3/product_backorders.csv")

# Specify the response and predictor variables
response <- "went_on_backorder"
predictors <- setdiff(names(train_df), response)

# Run AutoML specifying the stopping criterion
automl_models_h2o <- h2o.automl(
  x = predictors,
  y = response,
  training_frame = train_df,
  validation_frame = valid_df,
  max_runtime_secs = 60
)

# View the leaderboard
leaderboard <- automl_models_h2o@leaderboard
print(leaderboard)

# Extract the model from the leaderboard
model <- automl_models_h2o@leaderboard[6, "model_id"] %>%
  h2o.getModel()

# Extract the leader model
leader_model <- automl_models_h2o@leader

# Predict using the leader model
predictions <- h2o.predict(leader_model, newdata = test_df)
predictions_tbl <- as_tibble(predictions)
predictions_tbl

# Set the directory path to save the leader model
save_directory <- "C:/Users/mosta/Desktop/Business Decisions with Machine Learning/3/leadermodel"

# Save the leader model
h2o.saveModel(leader_model, path = save_directory, force = TRUE)
h2o.saveModel(model, path = save_directory, force = TRUE)

# Visualize the leaderboard
plot_h2o_leaderboard <- function(h2o_leaderboard, order_by = c("auc", "logloss"),
                                 n_max = 20, size = 4, include_lbl = TRUE) {
  # Setup inputs
  order_by <- tolower(order_by[[1]])
  
  leaderboard_tbl <- h2o_leaderboard %>%
    as_tibble() %>%
    select(-c(aucpr, mean_per_class_error, rmse, mse)) %>% 
    mutate(model_type = str_extract(model_id, "[^_]+")) %>%
    rownames_to_column(var = "rowname") %>%
    mutate(model_id = paste0(rowname, ". ", model_id) %>% as.factor())
  
  # Transformation
  if (order_by == "auc") {
    data_transformed_tbl <- leaderboard_tbl %>%
      slice(1:n_max) %>%
      mutate(
        model_id = as_factor(model_id) %>% reorder(auc),
        model_type = as.factor(model_type)
      ) %>%
      pivot_longer(cols = -c(model_id, model_type, rowname), 
                   names_to = "key", 
                   values_to = "value", 
                   names_transform = list(key = forcats::fct_inorder)
      )
  } else if (order_by == "logloss") {
    data_transformed_tbl <- leaderboard_tbl %>%
      slice(1:n_max) %>%
      mutate(
        model_id = as_factor(model_id) %>% reorder(logloss) %>% fct_rev(),
        model_type = as.factor(model_type)
      ) %>%
      pivot_longer(cols = -c(model_id, model_type, rowname), 
                   names_to = "key", 
                   values_to = "value", 
                   names_transform = list(key = forcats::fct_inorder)
      )
  } else {
    # If nothing is supplied
    stop(paste0("order_by = '", order_by, "' is not a permitted option."))
  }
  
  # Visualization
  g <- data_transformed_tbl %>%
    ggplot(aes(value, model_id, color = model_type)) +
    geom_point(size = size) +
    facet_wrap(~ key, scales = "free_x") +
    labs(title = "Leaderboard Metrics",
         subtitle = paste0("Ordered by: ", toupper(order_by)),
         y = "Model Position, Model ID", x = "")
  
  if (include_lbl) g <- g + geom_label(aes(label = round(value, 2), hjust = "inward"))
  
  return(g)
}

plot_h2o_leaderboard(automl_models_h2o@leaderboard, order_by = "auc", n_max = 15)


```


# 3

```{r plots ,fig.width=10, fig.height=10}
# Load the necessary libraries
library(h2o)
library(magrittr)
library(dplyr)
library(ggplot2)
library(tidyr)
library(tibble)
library(stringr)
library(forcats)

# Initialize H2O
h2o.init()

# Load the training, validation, and test datasets
train_df <- h2o.importFile(path = "C:/Users/mosta/Desktop/Business Decisions with Machine Learning/3/product_backorders.csv")
valid_df <- h2o.importFile(path = "C:/Users/mosta/Desktop/Business Decisions with Machine Learning/3/product_backorders.csv")
test_df <- h2o.importFile(path = "C:/Users/mosta/Desktop/Business Decisions with Machine Learning/3/product_backorders.csv")

# Specify the response and predictor variables
response <- "went_on_backorder"
predictors <- setdiff(names(train_df), response)

# Run AutoML specifying the stopping criterion
automl_models_h2o <- h2o.automl(
  x = predictors,
  y = response,
  training_frame = train_df,
  validation_frame = valid_df,
  max_runtime_secs = 60
)

# View the leaderboard
leaderboard <- automl_models_h2o@leaderboard
print(leaderboard)

# Extract the model from the leaderboard
model <- automl_models_h2o@leaderboard[6, "model_id"] %>%
  h2o.getModel()

# Extract the leader model
leader_model <- automl_models_h2o@leader

# Predict using the leader model
predictions <- h2o.predict(leader_model, newdata = test_df)
predictions_tbl <- as_tibble(predictions)
predictions_tbl

# Set the directory path to save the leader model
save_directory <- "C:/Users/mosta/Desktop/Business Decisions with Machine Learning/3/leadermodel"

# Save the leader model
h2o.saveModel(leader_model, path = save_directory, force = TRUE)
h2o.saveModel(model, path = save_directory, force = TRUE)

# Remove the leader model
leader_model <- NULL

# Define a unique grid_id using the current timestamp
grid_id_unique <- paste0("deeplearning_grid_", format(Sys.time(), "%Y%m%d%H%M%S"))

# Perform grid search with deeplearning algorithm
grid_search <- h2o.grid(
  algorithm = "deeplearning",
  grid_id = grid_id_unique,  # Use unique grid_id
  x = predictors,
  y = response,
  training_frame = train_df,
  validation_frame = valid_df,
  hyper_params = list(
    hidden = list(c(10, 10, 10), c(50, 20, 10), c(20, 20, 20)),
    epochs = c(10, 50, 100)
  )
)

# Get the best model from the grid search
grid_models <- h2o.getGrid(grid_search@grid_id)
best_model <- h2o.getModel(grid_models@model_ids[[1]])

# Predict using the best model
predictions_best <- h2o.predict(best_model, newdata = test_df)
predictions_best_tbl <- as_tibble(predictions_best)
predictions_best_tbl


```


# 4
```{r}
# Load the necessary libraries
library(h2o)
library(ggplot2)
library(dplyr)
library(tibble)

# Initialize H2O
h2o.init()

# Load the training, validation, and test datasets
train_df <- h2o.importFile(path = "C:/Users/mosta/Desktop/Business Decisions with Machine Learning/3/product_backorders.csv")
valid_df <- h2o.importFile(path = "C:/Users/mosta/Desktop/Business Decisions with Machine Learning/3/product_backorders.csv")
test_df <- h2o.importFile(path = "C:/Users/mosta/Desktop/Business Decisions with Machine Learning/3/product_backorders.csv")

# Specify the response and predictor variables
response <- "went_on_backorder"
predictors <- setdiff(names(train_df), response)

# Run AutoML specifying the stopping criterion
automl_models_h2o <- h2o.automl(
  x = predictors,
  y = response,
  training_frame = train_df,
  validation_frame = valid_df,
  max_runtime_secs = 60
)

# Extract the leader model
leader_model <- automl_models_h2o@leader

# Predict using the leader model
predictions <- h2o.predict(leader_model, newdata = test_df)
predictions_tbl <- as_tibble(predictions)

# Set the directory path to save the leader model
save_directory <- "C:/Users/mosta/Desktop/Business Decisions with Machine Learning/3/leadermodel"

# Save the leader model
h2o.saveModel(leader_model, path = save_directory, force = TRUE)

# Define new theme
theme_new <- theme(
  legend.position  = "bottom",
  legend.key       = element_blank(),
  panel.background = element_rect(fill   = "transparent"),
  panel.border     = element_rect(color = "black", fill = NA, size = 0.5),
  panel.grid.major = element_line(color = "grey", size = 0.333)
)


# Get the performance object for the leader model on the test set
perf <- h2o.performance(leader_model, newdata = test_df)

# Create a data frame of the recall and precision
pr_df <- data.frame(
  threshold = perf@metrics$thresholds_and_metric_scores$threshold,
  precision = perf@metrics$thresholds_and_metric_scores$precision,
  recall = perf@metrics$thresholds_and_metric_scores$recall
)

# Filter out any NA rows
pr_df <- pr_df[complete.cases(pr_df), ]

# Find the threshold that maximizes f1
optimal_threshold <- h2o.find_threshold_by_max_metric(perf, "f1")

# Plot the precision-recall curve with new theme
pr_df %>%
  ggplot(aes(x = threshold)) +
  geom_line(aes(y = precision), color = "blue", size = 1) +
  geom_line(aes(y = recall), color = "red", size = 1) +
  geom_vline(xintercept = optimal_threshold) +
  labs(title = "Precision vs Recall", y = "value") +
  theme_new

```



